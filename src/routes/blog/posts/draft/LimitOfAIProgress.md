---
title: 'What happens after AI progress reaches the singularity?'
date: '2025-10-11'
tags: ['Machine learning', 'Artificial General Intelligence']
shortDescription: 'Some postulations on the future of AI'
---

# Premise

At one of the farewell dinners in Baltimore before I embark on my journey to San Diego, one of my friends was asking whether I think AI will reach the singularity in our life time. While the precise definition within the context of the dinner was not clear so we devolved to random babbling, I had a lot of time to collect and organize my thoughts on this topic during my 6 days roadtrip to San Diego. The big question here is: **what would happen after AI reaches singularity?** More specially, I want to focus on shortly after AI reaching singularity, saying within this century.

# The scope of discussion

To guess what happens after AI reaches singularity, we first need to define what an AI reaching singularity means and what properties it has.
It is somewhat prevalant to hear people say once AI reaches singularity it will be infinitely powerful and essentially become an omnipotent god-like entity, that can advance science by thousands of years. I think this way of putting it muddies the waters. In the spirit of a short blog post, let's just keep three properties of an AI that has reached singularity:

1. Its capability and intelligence is beyond human comprehension.
2. It cannot be controlled by human.
2. It will be able to progress science infinitely fast.

My stand on the first property is that it is already true to some extent, and go no further than the famous AlphaGo beating Lee Sedol in 2016. And I think the beyond human comprehension part is not that important. There are many subjects and phenomena that we still do not have a complete understanding of, but we can still use them to our advantage. Anecdotally, a good portion of scientific discoveries were first made by empirical observations and experiments, and only later were explained theoretically, for example humans have been using stars for navigation for thousands of years before we understood what stars actually are. In the context of AI's capability, I don't think not being able to comprehend how AI works is of much importance. I am not saying this is not important, especially some people may argue AI is different because it is a constantly evolving entity so there is fundamentally no way to understand it, but still I think in terms of discussing the limit of AI progress, this is not the most important property.

The second property is an interesting one, and it usually goes hand in hand with the first property. By controlling, some imagine a steering wheel, that one's will is bending the direction of a car with it. But I argue this sense of control is an illusion, especially in cutting edge technology. Controllability is composed of our capability to interact with an external identity and our capability to predict what is going to happen after our actions, and thus it is more a spectrum than a binary. Here are a couple of examples:

1. I know whatever I do on Earth does not change the fact that the Sun is going to rise from the East tomorrow. Therefore I do not have control over the Sun.
2. When I flip a coin, I know it is going to land on either heads or tails, but I cannot predict which one it is going to be. Therefore I have limited control over the coin flip.
3. When I type on my computer, I know what is going to happen after I type a certain key. Therefore I have full control over my computer.

Now pay closer attention to the third example. Modern computers got really reliable over the last few decades, so it gives me a sense of complete control of the computers I use. But in reality, it is a matter of time that something goes wild then I lose control of my computer. I remember I had a laptop that I used for three years, that one day it just decide not to boot anymore despite nothing disastrous happened to it.

I think even an AI that has reached singularity is still going to be somewhat interactable and predictable, especially in scenarios which human has a good grip on how to solve a problem. **Hence, it is unrealistic to imagine a post-signularity AI to just do actions we cannot anticipate all the time.** The main reason I think people afraid of uncontrollable tools is our inherent adversion to uncertainty. There is some grey area in defining controllability since I left out the sense of "ownership" in my definition. But again, you can think you own something until you don't, cat owner can attest to this.

Finally, the property I want to focus on is the third one, that the AI can progress science infinitely fast. Let me just say I don't think this is true, and here is the blog post to explain why.


# Okay AI can improve itself, now what?

I think there is a misconception about what is going to happen after the AI can start to determininistically improve its performance, both in 

## Energy restriction

However efficient the machine might be, it cannot defy physical law.

## Data Restriction

## Rate of improvement

# Some big questions

## Will AI exterminate human like SkyNet?

On one hand life is very hard to exterminate, on the other hand the machine could be really good. Either way, if AI is to create misery, it is going to be a ton of misery

## Implication

